{"./":{"url":"./","title":"前言","keywords":"","body":"Python爬虫框架：PySpider 最新版本：v1.2 更新时间：20200815 简介 PySpider是一个简单易用且强大的Python主流爬虫框架。此处总结PySpider的安装和基本的使用，以及安装和启动时常见问题，并且给出查找定位元素的PyQuery的基本用法举例，以及一些高级用法，比如self.craw、config.json、data目录、phantomjs，和一些心得和常见的坑，且给出一些实际的例子供参考。 源码+浏览+下载 本书的各种源码、在线浏览地址、多种格式文件下载如下： Gitbook源码 crifan/python_spider_pyspider: Python爬虫框架：PySpider 如何使用此Gitbook源码去生成发布为电子书 详见：crifan/gitbook_template: demo how to use crifan gitbook template and demo 在线浏览 Python爬虫框架：PySpider book.crifan.com Python爬虫框架：PySpider crifan.github.io 离线下载阅读 Python爬虫框架：PySpider PDF Python爬虫框架：PySpider ePub Python爬虫框架：PySpider Mobi 版权说明 此电子书教程的全部内容，如无特别说明，均为本人原创和整理。其中部分内容参考自网络，均已备注了出处。如有发现侵犯您版权，请通过邮箱联系我 admin 艾特 crifan.com，我会尽快删除。谢谢合作。 鸣谢 感谢我的老婆陈雪的包容理解和悉心照料，才使得我crifan有更多精力去专注技术专研和整理归纳出这些电子书和技术教程，特此鸣谢。 更多其他电子书 本人crifan还写了其他100+本电子书教程，感兴趣可移步至： crifan/crifan_ebook_readme: Crifan的电子书的使用说明 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2021-01-17 12:01:35 "},"pyspider_intro/":{"url":"pyspider_intro/","title":"PySpider简介","keywords":"","body":"PySpider简介 PySpider的基本信息： 是个Python的爬虫框架 最大特点： 带图形界面WebUI的调试 简单易用 同时功能也很强大 GitHub https://github.com/binux/pyspider 文档： 官方，英文：http://docs.pyspider.org/ 非官方，中文：http://www.pyspider.cn/index.html 作者 网名：Binux 别名：足叉兆虫 博客：Binuxの杂货铺 Github: binux (Roy Binux) PySpider对比Scrapy 对于两个流行的Python的爬虫框架，PySpider和Scrapy，常常会被人拿来对比。 对此，之前简单总结如下： PySpider：简单易上手，带图形界面（基于浏览器页面） 一图胜千言：在WebUI中调试爬虫代码 Scrapy：可以高级定制化实现更加复杂的控制 一图胜千言：Scrapy一般是在命令行界面中调试页面返回数据： 详见： 【整理】pyspider vs scrapy crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 20:35:00 "},"pyspider_basic/":{"url":"pyspider_basic/","title":"PySpider安装与基本用法","keywords":"","body":"PySpider安装与基本用法 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 14:11:38 "},"pyspider_basic/pyspider_install/":{"url":"pyspider_basic/pyspider_install/","title":"PySpider安装","keywords":"","body":"PySpider安装 下面介绍，如何安装PySpider。 Mac中安装PySpider 此处以Mac中为例，去介绍如何安装PySpider。 如果只是简单的直接安装的话，则可以去： pip install pyspider 安装phantomjs 如果后续用到网页中要允许js的代码，则需要用到无头浏览器phantomjs，就需要先去安装phantomjs brew cask install phantomjs crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2020-08-15 11:26:54 "},"pyspider_basic/pyspider_install/common_issue.html":{"url":"pyspider_basic/pyspider_install/common_issue.html","title":"安装和启动的常见问题","keywords":"","body":"安装和启动的常见问题 此处整理在安装和启动pyspider期间，经常遇到的问题和其原因及解决办法。 启动报错：async=True SyntaxError: invalid syntax 运行pyspider报错： File \"/Users/limao/.pyenv/versions/3.8.0/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspider/run.py\", line 231 async=True, get_object=False, no_input=False): ^ SyntaxError: invalid syntax 原因：PySpider（很久没继续维护了）最新支持版本是Python 3.6，其把async作为普通函数参数，是没问题的。 但是Python 3.7之后把async改为了系统保留字，表示异步，所以asyc不能再作为普通函数参数名 所以当前环境是Python 3.7+时，就会报语法错误了 解决办法：2种思路： （自己）把代码（中这类的语法错误）都改掉 把当前Python版本换成旧版本：Python 3.6 此处选择换旧版本Python 3.6 步骤： Mac中换Python为3.6 此处以pyenv为例： 先去安装Python 3.6的某个版本： pyenv install 3.6.8 再去设置使用Python 3.6： 本地pyenv local 3.6.8 或全局pyenv global 3.6.8 然后重新安装： pip install pyspider 之后即可正常运行 pyspider 详见： 【已解决】Mac中给Python3安装PySpider 推荐用虚拟环境 在Python的开发中，为了避免不同开发环境的互相影响，一般都会用虚拟环境工具，比如pipenv、virtualenv。 注意：此处即使用Python虚拟环境，由于前面提到的问题，PySpider也还是要用Python 3.6的 pipenv中安装PySpider 此处是pipenv，为了指定使用Python 3.6则，可以在Pipfile中加上： [requires] python_version = \"3.6\" 贴出完整配置： [[source]] #url = \"https://pypi.python.org/simple\" url = \"https://pypi.tuna.tsinghua.edu.cn/simple\" verify_ssl = true name = \"pypi\" [packages] pymysql = \"*\" [dev-packages] [requires] python_version = \"3.6\" 创建并安装PySpider： pipenv install pyspider 或： 先创建虚拟环境，再安装PySpider pipenv install pipenv shell pip install pyspider virtualenv中安装PySpider 关于virtualenv： 创建虚拟环境：virtualenv venv 激活激活环境并进入： Mac/Linux：source venv/bin/activate 或： . venv/bin/activate Win：venv\\Scripts\\activate.bat 退出虚拟环境：deactivate 在virtualenv中安装PySpider： pip install pyspider 启动报错：pycurl报ImportError错 或 Curl报ConfigurationError错 Mac中，如果安装期间出错： __main__.ConfigurationError: Curl is configured to use SSL, but we have not been able to determine which SSL backend it is using 或运行时报错： ImportError: pycurl: libcurl link-time ssl backend (openssl) is different from compile-time ssl backend (none/other) 原因：此处导入pycurl时，发现libcurl运行时所依赖的ssl的底层是openssl，和当时编译时的版本不匹配 解决办法：重新编译安装，使得版本一致 步骤： pip uninstall -y pycurl export PYCURL_SSL_LIBRARY=openssl export LDFLAGS=-L/usr/local/opt/openssl/lib;export CPPFLAGS=-I/usr/local/opt/openssl/include;pip install pycurl --compile --no-cache-dir 注意：上述的： /usr/local/opt/openssl是你的openssl安装路径 如果你的不是这个路径， 要换成你的实际路径 对应的 /usr/local/opt/openssl/lib是lib库的路径 /usr/local/opt/openssl/include是include头文件的路径 详见： 【记录】Mac中安装和运行pyspider 【已解决】pipenv虚拟环境中用pip安装pyspider出错：、__main__.ConfigurationError: Curl is configured to use SSL, but we have not been able to determine which SSL backend it is using 【已解决】pyspider运行出错：ImportError pycurl libcurl link-time ssl backend (openssl) is different from compile-time ssl backend (none/other) 启动报错：fatal error openssl/ssl.h file not found 如果上面步骤： export LDFLAGS=-L/usr/local/opt/openssl/lib;export CPPFLAGS=-I/usr/local/opt/openssl/include;pip install pycurl --compile --no-cache-dir 报错： In file included from src/docstrings.c:4: src/pycurl.h:165:13: fatal error: 'openssl/ssl.h' file not found # include 1 error generated. error: command 'gcc' failed with exit status 1 直接原因：找不到openssl/ssl.h 多种可能 之前没安装过openssl 解决办法：Mac中去安装：brew install openssl 然后再重试即可 Mac中已安装过openssl 所以此处Mac中是有openssl/ssl.h的，只是传入的路径不对 解决办法：找到已安装的openssl的实际路径，传入正确的路径。 步骤： 找到已安装的openssl的实际安装路径 brew info openssl 可以看到有： /usr/local/Cellar/openssl@1.1/1.1.1d (7,983 files, 17.9MB) 其中的： /usr/local/Cellar/openssl@1.1/1.1.1d 就是我们要的，此处openssl的实际安装路径 通过 open /usr/local/Cellar/openssl@1.1/1.1.1d 确认是有对应的： /usr/local/Cellar/openssl@1.1/1.1.1d/include/openssl/ssl.h 这个文件的。所以传入路径应该改为： /usr/local/Cellar/openssl@1.1/1.1.1d/include 完整命令是： export LDFLAGS=-L/usr/local/opt/openssl/lib;export CPPFLAGS=-I/usr/local/Cellar/openssl@1.1/1.1.1d/include;pip install pycurl --compile --no-cache-dir 详见： 【已解决】Mac中pip安装pycurl报错：fatal error openssl/ssl.h file not found 启动报错：Error Could not create web server listening on port 25555 现象：运行pyspider时能看到有错误 Error: Could not create web server listening on port 25555 原因：之前已启动过pyspider，其内部会默认启动phantomjs，而虽然之前虽然已关闭掉pyspider，但是没有杀掉phantomjs的进程，导致端口25555被占用，而报错 解决办法：杀掉端口是25555的phantomjs进程即可 步骤： 找phantomjs进程ID： ps aux | grep 25555 杀掉对应进程 kill -9 xxx 举例 ✘ limao@xxx  ~/dev/crifan/python/demo_spider  ps aux | grep 25555 limao 35620 0.0 0.0 4277272 820 s002 R+ 10:27上午 0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn 25555 limao 33983 0.0 0.4 6130968 34128 s002 S 10:17上午 0:30.45 phantomjs --ssl-protocol=any --disk-cache=true /Users/limao/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspider/fetcher/phantomjs_fetcher.js 25555 limao@xxx  ~/dev/crifan/python/demo_spider  kill -9 33983 之后即可正常启动pyspider，且能看到phantomjs可以正常启动了： phantomjs fetcher running on port 25555 启动报错：Deprecated option domaincontroller use http_authenticator.domain_controller instead 启动报错： File \"/Users/limao/.pyenv/versions/3.6.8/lib/python3.6/site-packages/wsgidav/wsgidav_app.py\", line 118, in _check_config raise ValueError(\"Invalid configuration:\\n - \" + \"\\n - \".join(errors)) ValueError: Invalid configuration: - Deprecated option 'domaincontroller': use 'http_authenticator.domain_controller' instead. 原因：wsgidav版本兼容问题 解决办法：换兼容的没问题的旧版本2.4.1 步骤： pip install wsgidav==2.4.1 启动报错：ImportError cannot import name DispatcherMiddleware 启动报错： File \"/Users/limao/.pyenv/versions/3.6.8/lib/python3.6/site-packages/pyspider/webui/app.py\", line 64, in run from werkzeug.wsgi import DispatcherMiddleware ImportError: cannot import name 'DispatcherMiddleware' 原因：werkzeug版本兼容问题 解决办法：换兼容的没问题的旧版本0.16.1 步骤： pip install werkzeug==0.16.1 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2020-08-15 11:01:29 "},"pyspider_basic/pyspider_basic_usage/":{"url":"pyspider_basic/pyspider_basic_usage/","title":"PySpider基本用法","keywords":"","body":"PySpider基本用法 使用PySpider的基本步骤 下面来介绍一下PySpider的使用的步骤和操作： 运行PySpider 在某个目录下的终端命令行中输入 pyspider 即可启动运行，输出举例：  pyspider phantomjs fetcher running on port 25555 [I 200731 10:28:35 result_worker:49] result_worker starting... [I 200731 10:28:35 processor:211] processor starting... [I 200731 10:28:35 tornado_fetcher:638] fetcher starting... [I 200731 10:28:35 scheduler:647] scheduler starting... [I 200731 10:28:35 scheduler:782] scheduler.xmlrpc listening on 127.0.0.1:23333 [I 200731 10:28:35 scheduler:586] in 5m: new:0,success:0,retry:0,failed:0 [I 200731 10:28:35 app:84] webui exiting... 注： 如果是用虚拟环境安装的PySpider，记得先进去虚拟环境后再运行PySpider 比如用的pipenv，则是先pipenv shell，再pyspider pyspider等价于pyspider all 进入WebUI 然后去用浏览器打开： http://0.0.0.0:5000/ 即可进入爬虫的管理界面了，此界面一般称为WebUI 新建爬虫项目 点击Create，去新建一个爬虫项目 输入： 爬虫名称： 入口地址：自动生成的代码中，会作为起始要抓取的url 也可以不填 后续可以在代码中修改 然后再点击新建的爬虫项目，进入调试页面 新建出来的项目，默认状态是TODO 点击新建出来的项目名，直接进入调试界面 然后右边是编写代码的区域 左边是调试的区域，用于执行代码，显示输出信息等用途 调试爬虫代码 编写代码，调试输出信息，保存代码 调试代码期间，对于想要返回上一级： 先说之前不熟悉的时候的操作： 之前调试运行时，不知道还有回到上一级，在想要返回上一级时，都直接是点击左上角的项目名字 返回项目列表： 然后重新进去，重新点击Run，直到跑到对应的层级，去继续调试。 再说后来知道了PySpider内置支持这种逻辑操作： PySpider对在调试期间所需要在上一个连接和下一个连接之间切换的操作，支持的很好： 点击 的 或 >，则可以 返回上一级 或 进入下一级 实际效果演示： 想要返回上一级的爬取函数的话，点击 左箭头 然后再点击Run： 然后就可以返回上一级了。 然后也才注意到，每行的follow的左边开始显示的是：callback函数名 此处的是detail_page 而对应的上一级的结果中，也是上一级的callback： 运行爬虫去爬取数据 调试完毕后，返回项目，status改为DEBUG或RUNNING，点击Run 想要暂停运行：status改为STOP 保存已爬取的数据 当爬取完毕数据，需要保存下来时，可以有多种保存方式： mysql数据库 MongoDB数据库 CSV或Excel文件 保存到csv或Excel文件 基本思路：确保自己代码中，最后return返回的字段是你要的字段 如何得到CSV文件：在任务运行期间或完毕后，去Results-》点击下载CSV，即可得到你要的csv格式的数据文件。 结果：PySpider会自动在已有字段中加上额外的url字段 info:: 用VSCode编辑csv文件 如果想要去除多余的不需要的url字段，则可以通过文本编辑器，比如VSCode去列编辑模式，批量删除，或者查找和替换，都可以实现 最后会多余一列，标题是 …，内容全是,{}，所以直接用编辑器比如VCScode去替换为空以清空，即可 详见： 【已解决】PySpider如何把json结果数据保存到csv或excel文件中 – 在路上 warning:: Excel去打开CSV文件结果乱码 csv文件编码默认为UTF8（是好事，通用的），但是如果用（不论是Mac还是Win中的）excel去打开，结果（估计对于中文系统，都是）会默认以GKB（或GB18030）打开，所以会乱码 解决办法：【已解决】Mac或Win中用Excel打开UTF8编码的csv文件显示乱码 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2020-08-15 11:13:08 "},"pyspider_basic/pyspider_basic_usage/find_extract_html.html":{"url":"pyspider_basic/pyspider_basic_usage/find_extract_html.html","title":"查找提取元素","keywords":"","body":"PySpider中查找提取元素 PySpider中内置的用于查找和定位html网页中元素的库是：PyQuery PyQuery算是一个css选择器，模拟JS领域的jQuery，所以叫做PyQuery。 具体细节是，PySpider针对html的响应response，默认提供了一个doc属性，其内置了PyQuery解析后结果，所以你可以用response.doc(\"your_css_selector\")去选择你要的html中的内容了。 而具体的your_css_selector的写法，则就变成PyQuery的写法了。 举例：提取汽车之家车型车系相关数据 下面通过想象的例子来解释，PyQuery的常见用法： 比如想要提取： ... ... 图库 ... ... 中的href的值 语言描述可以是：class是rank-list-ul的ul元素，下面的li，下面的div，下面的a，且href值是包含/pic/series的 转换成PyQuery的语法是 .rank-list-ul li div a[href*=\"/pic/series\"] 也可以换成另外的写法： ul[class='rank-list-ul'] li div a[href*=\"/pic/series\"] ul[class='rank-list-ul'] a[href*=\"/pic/series\"] 如果你确定此规则不会误匹配其他元素，也可以省略中间的节点的查找 对应PySpider的代码是： 获取匹配到的第一个元素 firstMatchADoc = response.doc('.rank-list-ul li div a[href*=\"/pic/series\"]') 获取到所有匹配到的元素 for eachADoc in response.doc('.rank-list-ul li div a[href*=\"/pic/series\"]').items(): print(\"eachADoc=%s\" % eachADoc) 以及对于： 奥迪 一汽-大众奥迪 奥迪A3 指导价：19.32-23.46万 报价 图库 二手车 论坛 口碑 奥迪A4L 指导价：30.58-39.68万 报价 图库 二手车 论坛 口碑 ... ... ... 对应的代码是： 想要从 获取brand的logo的img的代码： brandDoc = response.doc('dl dt') brandLogoDoc = brandDoc.find('a img') brandLogoUrl = brandLogoDoc.attr[\"src\"] 从： 奥迪 中获取brand的name的a的代码： brandNameDoc = brandDoc.find('div a') brandName = brandNameDoc.text() 从： 一汽-大众奥迪 获取merchant的所有的a的代码： merchantDocGenerator = response.doc(\"dd div[class='h3-tit'] a\").items() merchantDocList = list(merchantDocGenerator) merchantDocLen = len(merchantDocList) 注意：.items()返回的是generator，想要得到list，需要用list(yourGenerator)去转换得到 从： ... 获取rank-list-ul的class的dd下面的ul的merchant的代码： merchantRankDocGenerator = response.doc(\"dd ul[class='rank-list-ul']\") merchantRankDocList = list(merchantRankDocGenerator) merchantRankDocListLen = len(merchantRankDocList) 以及获取每个元素： 属性值：用attr 类型是：dict 字符串值：用text() 类型是：str 举例： for curIdx, merchantItem in enumerate(merchantDocList): merchantName = merchantItem.text() merchantItemAttr = merchantItem.attr merchantUrl = merchantItemAttr[\"href\"] PyQuery资料 response.doc返回后的PyQuery对象，之后可以继续用PyQuery去操作 此处列出PyQuery的一些典型的操作函数： PyQuery.filter(selector) PyQuery.find(selector) PyQuery.items(selector=None) PyQuery.siblings(selector=None) 另外，常见的一些属性来说： PyQuery.text(value=)：当前节点的text文本值 PyQuery.html(value=, **kwargs)：当前节点的html值 详见： 官网文档 pyquery – PyQuery complete API — pyquery 1.2.4 documentation Traversing — pyquery 1.2.4 documentation Attributes — pyquery 1.2.4 documentation CSS — pyquery 1.2.4 documentation 独立教程 HTML解析库Python版jQuery：PyQuery crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2020-08-15 12:06:21 "},"pyspider_advanced/":{"url":"pyspider_advanced/","title":"PySpider的高级用法","keywords":"","body":"PySpider的高级用法 下面介绍PySpider中，除了基本用法之外的，可以算作是高级，稍微更加复杂一些的用法。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 15:55:26 "},"pyspider_advanced/self_crawl.html":{"url":"pyspider_advanced/self_crawl.html","title":"self.crawl","keywords":"","body":"self.crawl详解 官方文档 此处要介绍的PySpider的获取网络请求的函数是：self.crawl，功能很强大。 具体详细的解释，可以参考： 官网的英文文档： 比如： crawl参数 params 某热心网友整理的 中文文档： self.crawl - pyspider中文文档 - pyspider中文网 给GET的请求添加查询参数 给self.crawl中给params传递对应字典变量，PySpider内部会自动把字典编码为url的查询参数 query string. 官方实例： self.crawl('http://httpbin.org/get', callback=self.callback, params={'a': 123, 'b': 'c'}) 等价于： self.crawl('http://httpbin.org/get?a=123&b=c', callback=self.callback) 自己之前用的例子有： topSignTopParam = { \"start\": 0, \"rows\": 20 } self.crawl(TopSignTopUrl, callback=self.getMoreUserCallback, params=topSignTopParam, save={ \"baseUrl\": TopSignTopUrl, \"isNeedCheckNextPage\": True, \"curPageParam\": topSignTopParam } ) 给callback函数加上额外的参数 使用self.crawl的save参数即可，然后callback中用response.save获取传入的值 举例： def getUserDetail(self, userId): self.crawl(UserDetailUrl, callback=self.userDetailCallback, params={\"member_id\": userId }, save=userId ) def userDetailCallback(self, response): userId = response.save print(\"userId=%s\" % userId) 当请求出错时也执行callback回调函数 需要给callback回调函数加上修饰符@catch_status_code_error 举例： def picSeriesPage(self, response): ... self.crawl(curSerieDict[\"url\"], callback=self.carModelSpecPage, save=curSerieDict) @catch_status_code_error def carModelSpecPage(self, response): curSerieDict = response.save print(\"curSerieDict=%s\", curSerieDict) ... return curSerieDict crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 16:53:43 "},"pyspider_advanced/config_json.html":{"url":"pyspider_advanced/config_json.html","title":"config.json","keywords":"","body":"独立的配置文件: config.json 如果需要用到复杂一点的配置，比如result worker，则是需要单独写配置文件。 PySpider的配置文件一般叫做config.json 比如用如下内容： { \"taskdb\": \"mysql+taskdb://root:crifan_mysql@127.0.0.1:3306/AutohomeTaskdb\", \"projectdb\": \"mysql+projectdb://root:crifan_mysql@127.0.0.1:3306/AutohomeProjectdb\", \"resultdb\": \"mysql+resultdb://root:crifan_mysql@127.0.0.1:3306/AutohomeResultdb\", \"result_worker\":{ \"result_cls\": \"AutohomeResultWorker.AutohomeResultWorker\" } } 将config.json保存在pyspider命令运行所在的当前目录下： 然后去-c指定配置文件： pyspider -c config.json crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2019-05-31 20:31:54 "},"pyspider_advanced/data_folder.html":{"url":"pyspider_advanced/data_folder.html","title":"data目录","keywords":"","body":"PySpider所在目录下的data目录 在你运行pyspider后，自动会在命令执行路径下生成data文件夹，其中包含几个（SQLite）文件： project.db：保存了用户的爬虫项目相关信息，包括项目的Python代码 比如用（SQlite）工具去查看，可以看到详细数据 比如Mac中的DB Browser for SQLite查看的效果： Python代码： 对应数据库结构字段： result.db：项目运行的结果数据 task.db：项目相关的任务信息 其中如果开始运行爬虫，还会出现相关的调度信息： scheduler.all, scheduler.1d, scheduler.1h：保存了任务执行后所有，1天，1小时内相关的信息，和WebUI中的progress中的all, 1d, 1h对应： 如何清除之前的或正在运行的任务 对于一个写好的爬虫，且已经点击Run运行，或者运行了一段时间后，主动停止了。 接着想要去删除之前下载的数据，则： 官网的解释是： 设置group为delete，以及status为STOP后，过了(默认)24小时后，会自动删除该项目所有信息。 但是往往没法满足我们需求： 我不想要等待，只想现在就去：删除掉所有的信息，包括之前已经爬取的数据，之前的调度的任务等等数据。 经过一番研究后，发现了解决方案： 先去停止项目 WebUI中设置status为STOP 终端中用Control+C强制停止pyspider的运行 再去删除文件：result.db和task.db 如果还有任务相关的 scheduler.all，scheduler.1d，scheduler.1h，则一并删除 danger:: 不要轻易在没备份代码情况下删除project.db 注意不要删除，保存了项目（配置和）代码的：project.db，否则代码就没了。（我最开始就这么干过，&#x1F602;） 之后去重新运行pyspider，再去刷新WebUI界面： http://0.0.0.0:5000/ 即可看到干净的项目，没有了之前的任务和数据了。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 18:22:30 "},"pyspider_advanced/phantomjs.html":{"url":"pyspider_advanced/phantomjs.html","title":"phantomjs","keywords":"","body":"Phantomjs 如何解决部分页面内部不显示，无法抓取的问题？ 折腾【已解决】PySpider中页面部分内容不显示 – 在路上，遇到个问题： 页面中的部分内容不显示，所以无法抓取。 经过研究发现，其实是： 这部分不显示的内容，是原网页中通过后续调用js去生成和获取的，所以可以通过： 给self.crawl添加 fetch_type='js' 会使得内部调用phantomjs，模拟js，渲染生成页面内容。 从而，此处PySpider，在这种需要显示js加载的页面内容时，可以利用phantomjs。 用了phantomjs后又出错：FETCH_ERROR HTTP 599 Connection timed out after milliseconds 后续继续运行，加了fetch_type='js'的代码，去爬取页面数据，结果遇到了： 【未解决】pyspider运行出错：FETCH_ERROR HTTP 599 Connection timed out after milliseconds 尝试了多种办法，都无法解决此问题。 所以目前的情况是： 如果加了phantomjs，结果在大量爬取页面期间，又会导致出错FETCH_ERROR HTTP 599 Connection timed out after milliseconds，而暂时找不到解决办法。 给Phantomjs添加额外参数 之前折腾过： 【未解决】pyspider中如何给phantomjs传递额外参数 – 在路上 基本上没有实现想要的效果。但是可供参考。 phantomjs中的proxy是什么意思 对于pyspider来说，phantomjs-proxy参数指的是： 你另外所运行的phantomjs的实例 = host:port 比如： 在一个终端中运行： pyspider phantomjs --port 23450 --auto-restart true 然后去另外一个终端中运行pyspider： pyspider -c config.json all 其中config.json包含了： \"phantomjs-proxy\": \"127.0.0.1:23450\" 就可以使得此处的pyspider在启动时不另外启动phantomjs了。 而是去在需要用到phantomjs时，去连接本地电脑127.0.0.1的23450端口中的phantomjs去处理，去加载页面了。 而对于phantomjs本身来说： proxy，指的是代理，比如翻墙的代理，等等。 具体相关设置，可以参考： Command Line Interface | PhantomJS 中的： –proxy=address:port specifies the proxy server to use (e.g. –proxy=192.168.1.42:8080) –proxy-type=[http|socks5|none] specifies the type of the proxy server (default is http). –proxy-auth specifies the authentication information for the proxy, e.g. –proxy-auth=username:password) 详见：【已解决】pyspider中phantomjs中的proxy是什么意思 – 在路上 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 16:45:17 "},"pyspider_experience/":{"url":"pyspider_experience/","title":"PySpider经验与心得","keywords":"","body":"PySpider经验与心得 折腾了一些PySpider项目，有些经验和心得，整理如下： crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 17:21:43 "},"pyspider_experience/summary_note.html":{"url":"pyspider_experience/summary_note.html","title":"PySpider的心得","keywords":"","body":"PySpider的心得 对于加载更多内容，除了想办法找js或api，也可以换个其他的思路 问题：想要获取单个页面的更多的内容，一般页面都是向下滚动，加载更多。内部往往是js实现，调用额外的api获取更多数据，加载更多数据。 思路：所以一般往往会去研究和抓包，搞清楚调用的api。但是其实有思路多去看看网页中与之相关的其他内容，往往可以通过其他途径，比如另外有个单独的页面，可以获取我所需要的所有的车型车系的数据。就可以避免非要去研究和抓包api了。 详见：【已解决】pyspider中如何加载汽车之家页面中的更多内容 调试界面中的enable css selector helper 点击web后可以看到html页面内容 再点击enable css selector helper后 之后点击某个页面元素，则可以直接显示出对应的css的selector 不过话说我个人调试页面期间，很少用到。 都是直接去Chrome浏览器中调试页面，查看html源码，寻找合适的css selector。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 18:45:03 "},"pyspider_experience/pitfall.html":{"url":"pyspider_experience/pitfall.html","title":"PySpider常见的坑","keywords":"","body":"PySpider常见的坑 关于折腾PySpider期间，遇到很多或大或小的坑，常见和具体细节相关的坑，已记录到对应部分中了。 此处再继续整理出，其他的一些常见的坑。 css的选择器不工作 背景：网页中的源码本来是： 或者类似的： href=\"/pic/series-t/3170.html\" 然后去写css选择器： a[href^=\"//car.autohome.com.cn/pic/series/\"] 但是却无法匹配 原因：PySpider内部的css选择器用的是PyQuery，其默认把href的路径，加上了对应的host，所以此时获取到的html实际上变成了： 详见： response.doc Reponse.doc() 返回的就是一个PyQuery的对象 Links have made as absolute by default 猜测：估计是为了方便小白用户，所以默认加上了host，但是坑了其他人啊。 解决办法：此处被逼的css选择器写法只能改为： a[href*=\"pic/series/\"] 或类似的代码： fnRightPicSeries = response.doc('.search-pic-tbar .fn-right a[href*=\"/pic/series\"]') fullPicSeriesUrl = fnRightPicSeries.attr.href 已经得到的是，加了host/domain的绝对路径了： fullPicSeriesUrl= https://car.autohome.com.cn/pic/series-t/3170.html 详见： 【已解决】pyspider中的css选择器不工作 – 在路上 Error Could not create web server listening on port 25555 原因：对应的25555端口被占用了 根本原因：之前的PySpider没有正常的彻底的被关闭，所以残留了。 解决办法：彻底kill干掉之前的PySpider的进程即可。 举例： 普通Linux类系统，用： 找到占了25555端口的进程的id：ps aux | grep 25555 再去杀掉进程：kill process_id -9 即可。 如果是Mac中，则用lsof ➜ AutocarData lsof -i:25555 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME phantomjs 46971 crifan 12u IPv4 0xe4d24cdcaf5e481f 0t0 TCP *:25555 (LISTEN) ➜ AutocarData kill 46971 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2019-05-31 20:31:54 "},"pyspider_example/":{"url":"pyspider_example/","title":"PySpider案例","keywords":"","body":"PySpider案例 下面把一些之前写过的爬虫分享出来，供参考： 爬取汽车之家的品牌车型车型数据 文件：autohomeBrandData.py #!/usr/bin/env python # -*- encoding: utf-8 -*- # Created on 2018-04-27 21:53:02 # Project: autohomeBrandData from pyspider.libs.base_handler import * import string import re class Handler(BaseHandler): crawl_config = { } # @every(minutes=24 * 60) def on_start(self): for eachLetter in list(string.ascii_lowercase): self.crawl(\"https://www.autohome.com.cn/grade/carhtml/%s.html\" % eachLetter, callback=self.gradCarHtmlPage) @catch_status_code_error def gradCarHtmlPage(self, response): print(\"gradCarHtmlPage: response=\", response) picSeriesItemList = response.doc('.rank-list-ul li div a[href*=\"/pic/series\"]').items() print(\"picSeriesItemList=\", picSeriesItemList) # print(\"len(picSeriesItemList)=%s\"%(len(picSeriesItemList))) for each in picSeriesItemList: self.crawl(each.attr.href, callback=self.picSeriesPage) @config(priority=2) def picSeriesPage(self, response): # 查看停产车型&nbsp;&gt; # 查看在售车型&nbsp;&gt; # &nbsp; fnRightPicSeries = response.doc('.search-pic-tbar .fn-right a[href*=\"/pic/series\"]') print(\"fnRightPicSeries=\", fnRightPicSeries) if fnRightPicSeries: # hrefValue = fnRightPicSeries.attr.href # print(\"hrefValue=\", hrefValue) # fullPicSeriesUrl = \"https://car.autohome.com.cn\" + hrefValue fullPicSeriesUrl = fnRightPicSeries.attr.href print(\"fullPicSeriesUrl=\", fullPicSeriesUrl) self.crawl(fullPicSeriesUrl, callback=self.picSeriesPage) # contine parse brand data aDictList = [] # for eachA in response.doc('.breadnav a[href^=\"/\"]').items(): for eachA in response.doc('.breadnav a[href*=\"/pic/\"]').items(): eachADict = { \"text\": eachA.text(), \"href\": eachA.attr.href } print(\"eachADict=\", eachADict) aDictList.append(eachADict) print(\"aDictList=\", aDictList) mainBrandDict = aDictList[-3] subBrandDict = aDictList[-2] brandSerieDict = aDictList[-1] print(\"mainBrandDict=%s, subBrandDict=%s, brandSerieDict=%s\" % (mainBrandDict, subBrandDict, brandSerieDict)) dtTextList = [] for eachDt in response.doc(\"dl.search-pic-cardl dt\").items(): dtTextList.append(eachDt.text()) print(\"dtTextList=\", dtTextList) groupCount = len(dtTextList) print(\"groupCount=\", groupCount) for eachDt in response.doc(\"dl.search-pic-cardl dt\").items(): dtTextList.append(eachDt.text()) ddUlEltList = [] for eachDdUlElt in response.doc(\"dl.search-pic-cardl dd ul\").items(): ddUlEltList.append(eachDdUlElt) print(\"ddUlEltList=\", ddUlEltList) modelDetailDictList = [] for curIdx in range(groupCount): curGroupTitle = dtTextList[curIdx] print(\"------[%d] %s\" % (curIdx, curGroupTitle)) for eachLiAElt in ddUlEltList[curIdx].items(\"li a\"): # 1. model name # curModelName = eachLiAElt.text() curModelName = eachLiAElt.contents()[0] curModelName = curModelName.strip() print(\"curModelName=\", curModelName) curFullModelName = curGroupTitle + \" \" + curModelName print(\"curFullModelName=\", curFullModelName) # 2. model id + carSeriesId + spec url curModelId = \"\" curSeriesId = \"\" curModelSpecUrl = \"\" modelSpecUrlTemplate = \"https://www.autohome.com.cn/spec/%s/#pvareaid=2042128\" curModelPicUrl = eachLiAElt.attr.href print(\"curModelPicUrl=\", curModelPicUrl) # https://car.autohome.com.cn/pic/series-s32708/3457.html#pvareaid=2042220 foundModelSeriesId = re.search(\"pic/series-s(?P\\d+)/(?P\\d+)\\.html\", curModelPicUrl) print(\"foundModelSeriesId=\", foundModelSeriesId) if foundModelSeriesId: curModelId = foundModelSeriesId.group(\"curModelId\") curSeriesId = foundModelSeriesId.group(\"curSeriesId\") print(\"curModelId=%s, curSeriesId=%s\", curModelId, curSeriesId) curModelSpecUrl = (modelSpecUrlTemplate) % (curModelId) print(\"curModelSpecUrl=\", curModelSpecUrl) # 3. model status modelStatus = \"在售\" foundStopSale = eachLiAElt.find('i[class*=\"icon-stopsale\"]') if foundStopSale: modelStatus = \"停售\" else: foundWseason = eachLiAElt.find('i[class*=\"icon-wseason\"]') if foundWseason: modelStatus = \"未上市\" modelDetailDictList.append({ \"url\": curModelSpecUrl, # \"车系ID\": curSeriesId, # \"车型ID\": curModelId, # \"车型\": curFullModelName, # \"状态\": modelStatus \"brandSerieId\": curSeriesId, \"modelId\": curModelId, \"model\": curFullModelName, \"modelStatus\": modelStatus }) print(\"modelDetailDictList=\", modelDetailDictList) allSerieDictList = [] for curIdx, eachModelDetailDict in enumerate(modelDetailDictList): \"\"\" defined in mysql CREATE TABLE `tbl_autohome_car_info` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '自增，主键', `cityDealerPrice` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '经销商参考价', `msrpPrice` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '厂商指导价', `mainBrand` char(20) NOT NULL DEFAULT '' COMMENT '品牌', `subBrand` varchar(20) NOT NULL DEFAULT '' COMMENT '子品牌', `brandSerie` varchar(20) NOT NULL DEFAULT '' COMMENT '车系', `brandSerieId` varchar(15) NOT NULL DEFAULT '' COMMENT '车系ID', `model` varchar(50) NOT NULL DEFAULT '' COMMENT '车型', `modelId` varchar(15) NOT NULL DEFAULT '' COMMENT '车型ID', `modelStatus` char(5) NOT NULL DEFAULT '' COMMENT '车型状态', `url` varchar(200) NOT NULL DEFAULT '' COMMENT '车型url', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; \"\"\" curSerieDict = { \"url\": eachModelDetailDict[\"url\"], # \"品牌\": mainBrandDict[\"text\"], # \"子品牌\": subBrandDict[\"text\"], # \"车系\": brandSerieDict[\"text\"], # \"车系ID\": eachModelDetailDict[\"车系ID\"], # \"车型\": eachModelDetailDict[\"车型\"], # \"车型ID\": eachModelDetailDict[\"车型ID\"], # \"状态\": eachModelDetailDict[\"状态\"] \"mainBrand\": mainBrandDict[\"text\"], \"subBrand\": subBrandDict[\"text\"], \"brandSerie\": brandSerieDict[\"text\"], \"brandSerieId\": eachModelDetailDict[\"brandSerieId\"], \"model\": eachModelDetailDict[\"model\"], \"modelId\": eachModelDetailDict[\"modelId\"], \"modelStatus\": eachModelDetailDict[\"modelStatus\"] } allSerieDictList.append(curSerieDict) # print(\"before send_message: [%d] curSerieDict=%s\" % (curIdx, curSerieDict)) # self.send_message(self.project_name, curSerieDict, url=eachModelDetailDict[\"url\"]) print(\"[%d] curSerieDict=%s\" % (curIdx, curSerieDict)) self.crawl(eachModelDetailDict[\"url\"], callback=self.carModelSpecPage, fetch_type='js', retries=5, connect_timeout=50, timeout=300, save=curSerieDict) # print(\"allSerieDictList=\", allSerieDictList) # return allSerieDictList # def on_message(self, project, msg): # print(\"on_message: msg=\", msg) # return msg @catch_status_code_error def carModelSpecPage(self, response): print(\"carModelSpecPage: response=\", response) # https://www.autohome.com.cn/spec/32708/#pvareaid=2042128 curSerieDict = response.save print(\"curSerieDict\", curSerieDict) cityDealerPriceInt = 0 cityDealerPriceElt = response.doc('.cardetail-infor-price #cityDealerPrice span span[class*=\"price\"]') print(\"cityDealerPriceElt=%s\" % cityDealerPriceElt) if cityDealerPriceElt: cityDealerPriceFloatStr = cityDealerPriceElt.text() print(\"cityDealerPriceFloatStr=\", cityDealerPriceFloatStr) cityDealerPriceFloat = float(cityDealerPriceFloatStr) print(\"cityDealerPriceFloat=\", cityDealerPriceFloat) cityDealerPriceInt = int(cityDealerPriceFloat * 10000) print(\"cityDealerPriceInt=\", cityDealerPriceInt) msrpPriceInt = 0 # body > div.content > div.row > div.column.grid-16 > div.cardetail.fn-clear > div.cardetail-infor > div.cardetail-infor-price.fn-clear > ul > li.li-price.fn-clear > span # 厂商指导价=厂商建议零售价格=MSRP=Manufacturer's suggested retail price msrpPriceElt = response.doc('.cardetail-infor-price li[class*=\"li-price\"] span[data-price]') print(\"msrpPriceElt=\", msrpPriceElt) if msrpPriceElt: msrpPriceStr = msrpPriceElt.attr(\"data-price\") print(\"msrpPriceStr=\", msrpPriceStr) foundMsrpPrice = re.search(\"(?P[\\d\\.]+)万元\", msrpPriceStr) print(\"foundMsrpPrice=\", foundMsrpPrice) if foundMsrpPrice: msrpPrice = foundMsrpPrice.group(\"msrpPrice\") print(\"msrpPrice=\", msrpPrice) msrpPriceFloat = float(msrpPrice) print(\"msrpPriceFloat=\", msrpPriceFloat) msrpPriceInt = int(msrpPriceFloat * 10000) print(\"msrpPriceInt=\", msrpPriceInt) # curSerieDict[\"经销商参考价\"] = cityDealerPriceInt # curSerieDict[\"厂商指导价\"] = msrpPriceInt curSerieDict[\"cityDealerPrice\"] = cityDealerPriceInt curSerieDict[\"msrpPrice\"] = msrpPriceInt return curSerieDict 文件：AutohomeResultWorker.py #!/usr/bin/env python # -*- encoding: utf-8 -*- # Project: autohomeBrandData # Function: implement custom result worker for autohome car data # Author: Crifan Li # Date: 20180512 # Note: # If you want to modify to your mysql and table, you need: # (1) change change MysqlDb config to your mysql config # (2) change CurrentTableName to your table name # (3) change CreateTableSqlTemplate to your sql to create new mysql table fields # (4) before use this ResultWorker, run py file to execute testMysqlDb, to init db and create table # (5) if your table field contain more type, edit insert to add more type for \"TODO: add more type formatting if necessary\" import pymysql import pymysql.cursors from pyspider.result import ResultWorker CurrentTableName = \"tbl_autohome_car_info\" CreateTableSqlTemplate = \"\"\"CREATE TABLE IF NOT EXISTS `%s` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '自增，主键', `cityDealerPrice` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '经销商参考价', `msrpPrice` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '厂商指导价', `mainBrand` char(20) NOT NULL DEFAULT '' COMMENT '品牌', `subBrand` varchar(20) NOT NULL DEFAULT '' COMMENT '子品牌', `brandSerie` varchar(20) NOT NULL DEFAULT '' COMMENT '车系', `brandSerieId` varchar(15) NOT NULL DEFAULT '' COMMENT '车系ID', `model` varchar(50) NOT NULL DEFAULT '' COMMENT '车型', `modelId` varchar(15) NOT NULL DEFAULT '' COMMENT '车型ID', `modelStatus` char(5) NOT NULL DEFAULT '' COMMENT '车型状态', `url` varchar(200) NOT NULL DEFAULT '' COMMENT '车型url', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;\"\"\" class AutohomeResultWorker(ResultWorker): def __init__(self, resultdb, inqueue): \"\"\"init mysql db\"\"\" print(\"AutohomeResultWorker init: resultdb=%s, inqueue=%s\" % (resultdb, inqueue)) ResultWorker.__init__(self, resultdb, inqueue) self.mysqlDb = MysqlDb() print(\"self.mysqlDb=%s\" % self.mysqlDb) def on_result(self, task, result): \"\"\"override pyspider on_result to save data into mysql\"\"\" # assert task['taskid'] # assert task['project'] # assert task['url'] # assert result print(\"AutohomeResultWorker on_result: task=%s, result=%s\" % (task, result)) insertOk = self.mysqlDb.insert(result) print(\"insertOk=%s\" % insertOk) class MysqlDb: config = { 'host': '127.0.0.1', 'port': 3306, 'user': 'root', 'password': 'crifan_mysql', 'database': 'AutohomeResultdb', 'charset': \"utf8\" } defaultTableName = CurrentTableName connection = None def __init__(self): \"\"\"init mysql\"\"\" # 1. connect db first if self.connection is None: isConnected = self.connect() print(\"Connect mysql return %s\" % isConnected) # 2. create table for db createTableOk = self.createTable(self.defaultTableName) print(\"Create table %s return %s\" %(self.defaultTableName, createTableOk)) def connect(self): try: self.connection = pymysql.connect(**self.config, cursorclass=pymysql.cursors.DictCursor) print(\"connect mysql ok, self.connection=\", self.connection) return True except pymysql.Error as err: print(\"Connect mysql with config=\", self.config, \" error=\", err) return False def quoteIdentifier(self, identifier): \"\"\" for mysql, it better to quote identifier xxx using backticks to `xxx` in case, identifier: contain special char, such as space or same with system reserved words, like select \"\"\" quotedIdentifier = \"`%s`\" % identifier # print(\"quotedIdentifier=\", quotedIdentifier) return quotedIdentifier def executeSql(self, sqlStr, actionDescription=\"\"): print(\"executeSql: sqlStr=%s, actionDescription=%s\" % (sqlStr, actionDescription)) if self.connection is None: print(\"Please connect mysql first before %s\" % actionDescription) return False cursor = self.connection.cursor() print(\"cursor=\", cursor) try: cursor.execute(sqlStr) self.connection.commit() print(\"+++ Ok to execute sql %s for %s\" % (sqlStr, actionDescription)) return True except pymysql.Error as err: print(\"!!! %s when execute sql %s for %s\" % (err, sqlStr, actionDescription)) return False def createTable(self, newTablename): print(\"createTable: newTablename=\", newTablename) createTableSql = CreateTableSqlTemplate % (newTablename) print(\"createTableSql=\", createTableSql) return self.executeSql(sqlStr=createTableSql, actionDescription=(\"Create table %s\" % newTablename)) def dropTable(self, existedTablename): print(\"dropTable: existedTablename=\", existedTablename) dropTableSql = \"DROP TABLE IF EXISTS %s\" % (existedTablename) print(\"dropTableSql=\", dropTableSql) return self.executeSql(sqlStr=dropTableSql, actionDescription=(\"Drop table %s\" % existedTablename)) # def insert(self, **valueDict): def insert(self, valueDict, tablename=defaultTableName): \"\"\" inset dict value into mysql table makesure the value is dict, and its keys is the key in the table \"\"\" print(\"insert: valueDict=%s, tablename=%s\" % (valueDict, tablename)) dictKeyList = valueDict.keys() dictValueList = valueDict.values() print(\"dictKeyList=\", dictKeyList, \"dictValueList=\", dictValueList) keyListSql = \", \".join(self.quoteIdentifier(eachKey) for eachKey in dictKeyList) print(\"keyListSql=\", keyListSql) # valueListSql = \", \".join(eachValue for eachValue in dictValueList) valueListSql = \"\" formattedDictValueList = [] for eachValue in dictValueList: # print(\"eachValue=\", eachValue) eachValueInSql = \"\" valueType = type(eachValue) # print(\"valueType=\", valueType) if valueType is str: eachValueInSql = '\"%s\"' % eachValue elif valueType is int: eachValueInSql = '%d' % eachValue # TODO: add more type formatting if necessary print(\"eachValueInSql=\", eachValueInSql) formattedDictValueList.append(eachValueInSql) valueListSql = \", \".join(eachValue for eachValue in formattedDictValueList) print(\"valueListSql=\", valueListSql) insertSql = \"\"\"INSERT INTO %s (%s) VALUES (%s)\"\"\" % (tablename, keyListSql, valueListSql) print(\"insertSql=\", insertSql) # INSERT INTO tbl_car_info_test (`url`, `mainBrand`, `subBrand`, `brandSerie`, `brandSerieId`, `model`, `modelId`, `modelStatus`, `cityDealerPrice`, `msrpPrice`) VALUES (\"https://www.autohome.com.cn/spec/5872/#pvareaid=2042128\", \"宝马\", \"华晨宝马\", \"宝马3系\", \"66\", \"2010款 320i 豪华型\", \"5872\", \"停售\", 325000, 375000) return self.executeSql(sqlStr=insertSql, actionDescription=(\"Insert value to table %s\" % tablename)) def delete(self, modelId, tablename=defaultTableName): \"\"\" delete item from car model id for existing table of autohome car info \"\"\" print(\"delete: modelId=%s, tablename=%s\" % (modelId, tablename)) deleteSql = \"\"\"DELETE FROM %s WHERE modelId = %s\"\"\" % (tablename, modelId) print(\"deleteSql=\", deleteSql) return self.executeSql(sqlStr=deleteSql, actionDescription=(\"Delete value from table %s by model id %s\" % (tablename, modelId))) def testMysqlDb(): \"\"\"test mysql\"\"\" testDropTable = True testCreateTable = True testInsertValue = True testDeleteValue = True # 1.test connect mysql mysqlObj = MysqlDb() print(\"mysqlObj=\", mysqlObj) # testTablename = \"autohome_car_info\" # testTablename = \"tbl_car_info_test\" testTablename = CurrentTableName print(\"testTablename=\", testTablename) if testDropTable: # 2. test drop table dropTableOk = mysqlObj.dropTable(testTablename) print(\"dropTable\", testTablename, \"return\", dropTableOk) if testCreateTable: # 3. test create table createTableOk = mysqlObj.createTable(testTablename) print(\"createTable\", testTablename, \"return\", createTableOk) if testInsertValue: # 4. test insert value dict valueDict = { \"url\": \"https://www.autohome.com.cn/spec/5872/#pvareaid=2042128\", #车型url \"mainBrand\": \"宝马\", #品牌 \"subBrand\": \"华晨宝马\", #子品牌 \"brandSerie\": \"宝马3系\", #车系 \"brandSerieId\": \"66\", #车系ID \"model\": \"2010款 320i 豪华型\", #车型 \"modelId\": \"5872\", #车型ID \"modelStatus\": \"停售\", #车型状态 \"cityDealerPrice\": 325000, #经销商参考价 \"msrpPrice\": 375000 # 厂商指导价 } print(\"valueDict=\", valueDict) insertOk = mysqlObj.insert(valueDict=valueDict, tablename=testTablename) print(\"insertOk=\", insertOk) if testDeleteValue: toDeleteModelId = \"5872\" deleteOk = mysqlObj.delete(modelId=toDeleteModelId, tablename=testTablename) print(\"deleteOk=\", deleteOk) def testAutohomeResultWorker(): \"\"\"just test for create mysql db is ok or not\"\"\" autohomeResultWorker = AutohomeResultWorker(None, None) print(\"autohomeResultWorker=%s\" % autohomeResultWorker) if __name__ == '__main__': testMysqlDb() # testAutohomeResultWorker() 配置文件：config.json { \"resultdb\": \"mysql+resultdb://root:crifan_mysql@127.0.0.1:3306/AutohomeResultdb\", \"result_worker\":{ \"result_cls\": \"AutohomeResultWorker.AutohomeResultWorker\" }, \"phantomjs-proxy\": \"127.0.0.1:23450\", \"phantomjs\" : { \"port\": 23450, \"auto-restart\": true, \"load-images\": false, \"debug\": true } } crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 20:17:00 "},"appendix/":{"url":"appendix/","title":"附录","keywords":"","body":"附录 下面列出相关参考资料。 crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2018-09-20 13:57:58 "},"appendix/reference.html":{"url":"appendix/reference.html","title":"参考资料","keywords":"","body":"参考资料 【已解决】Mac中给Python3安装PySpider 【已解决】Mac中安装phantomjs 【已解决】Mac中pip安装pycurl报错：fatal error openssl/ssl.h file not found HTML解析库Python版jQuery：PyQuery pyspider是开源强大的python爬虫系统 - pyspider中文网 【已解决】PySpider中保存数据到mysql 【已解决】PySpider中如何清空之前运行的数据和正在运行的任务 – 在路上 【未解决】pyspider中如何给phantomjs传递额外参数 – 在路上 【已解决】PySpider中页面部分内容不显示 – 在路上 【未解决】pyspider运行出错：FETCH_ERROR HTTP 599 Connection timed out after milliseconds 【记录】Mac中安装和运行pyspider 【整理】pyspider vs scrapy 【已解决】pyspider中phantomjs中的proxy是什么意思 – 在路上 【已解决】pyspider中运行result_worker出错：ModuleNotFoundError No module named mysql 【已解决】PySpider中传递参数给下一级且当下一级失败时也可以执行 【已解决】pyspider中出错：TypeError __init__() got an unexpected keyword argument resultdb – 在路上 【已解决】pyspider运行出错：ImportError pycurl libcurl link-time ssl backend (openssl) is different from compile-time ssl backend (none/other) 【已解决】pyspider中pymysql中insert失败且except部分代码没有执行 【已解决】pyspider运行出错：Error Could not create web server listening on port 25555 – 在路上 【已解决】pyspider中的css选择器不工作 – 在路上 【已解决】pyspider中如何写规则去提取网页内容 – 在路上 【已解决】PySpider如何把json结果数据保存到csv或excel文件中 – 在路上 【已解决】PySpider中如何单个页面返回多个json数据结果 – 在路上 【已解决】Mac或Win中用Excel打开UTF8编码的csv文件显示乱码 安装pyspider遇到的坑（python3.6）_盛夏88688的博客-CSDN博客_python 3.6 with报错use async with instead crifan.com，使用署名4.0国际(CC BY 4.0)协议发布 all right reserved，powered by Gitbook最后更新： 2020-08-15 12:06:29 "}}